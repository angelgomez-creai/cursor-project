---
title: "M√≥dulo 6: DevOps y Deployment con Cursor AI"
description: Containeriza aplicaciones con Docker, automatiza CI/CD pipelines, despliega en cloud, implementa Infrastructure as Code y monitoring empresarial para sistemas en producci√≥n.
---

import { Card, CardGrid } from '@astrojs/starlight/components';

## Objetivos de Aprendizaje

Al finalizar este m√≥dulo, ser√°s capaz de:

1. **Containerizar aplicaciones completas** con Docker multi-stage, optimizaci√≥n y security best practices
2. **Automatizar CI/CD pipelines** con GitHub Actions, testing, security scanning y deployment autom√°tico
3. **Desplegar en cloud** usando AWS/GCP con Infrastructure as Code (Terraform) y auto-scaling
4. **Implementar monitoring completo** con Prometheus, Grafana, alertas y observabilidad empresarial
5. **Gestionar secretos y seguridad** con encrypted storage, secret rotation y compliance
6. **Configurar estrategias de deployment** con blue-green, canary deployments y rollback autom√°tico

## üìπ Videos del M√≥dulo

### üéØ Video Principal: DevOps y Deployment

Domina deployment autom√°tico y CI/CD con Docker y cloud services.

<iframe 
  width="100%" 
  height="400" 
  src="https://www.youtube.com/embed/rGawSLRI_Oo" 
  title="DevOps y Deployment con Cursor AI" 
  frameborder="0" 
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
  allowfullscreen
  style="border-radius: 8px; margin: 1rem 0;">
</iframe>

### üõ†Ô∏è Video Complementario: Containerizaci√≥n

Aprende Docker y containerizaci√≥n para aplicaciones modernas.

<iframe 
  width="100%" 
  height="400" 
  src="https://www.youtube.com/embed/bAAbrhb3QoM" 
  title="Docker y Containerizaci√≥n" 
  frameborder="0" 
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
  allowfullscreen
  style="border-radius: 8px; margin: 1rem 0;">
</iframe>

### üöÄ Video Avanzado: Infrastructure as Code

Implementa infraestructura como c√≥digo con Terraform y herramientas modernas.

<iframe 
  width="100%" 
  height="400" 
  src="https://www.youtube.com/embed/fGQ_gl_RQ_M" 
  title="Infrastructure as Code" 
  frameborder="0" 
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
  allowfullscreen
  style="border-radius: 8px; margin: 1rem 0;">
</iframe>

## Introducci√≥n Conceptual

DevOps moderno va m√°s all√° de "subir c√≥digo a producci√≥n"‚Äîse trata de **arquitectar sistemas que se despliegan, escalan y mantienen autom√°ticamente**. Este m√≥dulo te ense√±a a usar Cursor AI para transformar tu aplicaci√≥n en un **sistema enterprise-ready** que opera 24/7 con observabilidad completa.

La **containerizaci√≥n con Docker** y **Infrastructure as Code** son est√°ndares para aplicaciones modernas porque resuelven los problemas fundamentales del deployment: **reproducibilidad**, **escalabilidad**, **observabilidad** y **recuperaci√≥n autom√°tica**. Aprender√°s a implementar estas pr√°cticas usando Cursor AI como tu copiloto de DevOps.

Al final de este m√≥dulo, habr√°s desplegado tu aplicaci√≥n e-commerce en **producci√≥n real** con monitoring avanzado, auto-scaling, backup autom√°tico y estrategias de disaster recovery que pueden manejar miles de usuarios concurrentes.

## Docker y Containerizaci√≥n

### ¬øPor qu√© Docker?

<CardGrid>
  <Card title="üì¶ Portabilidad" icon="rocket">
    "Funciona en mi m√°quina" ‚Üí "Funciona en cualquier lado"
  </Card>
  
  <Card title="üîß Consistencia" icon="setting">
    Mismo ambiente en desarrollo, staging y producci√≥n
  </Card>
  
  <Card title="‚ö° Escalabilidad" icon="star">
    F√°cil escalar horizontalmente con orchestrators
  </Card>
  
  <Card title="üîí Aislamiento" icon="lock">
    Aplicaciones aisladas con dependencies separadas
  </Card>
</CardGrid>

### Docker para Backend (FastAPI)

#### Dockerfile Optimizado para Backend

```dockerfile
# backend/Dockerfile
FROM python:3.11-slim as base

# Establecer directorio de trabajo
WORKDIR /app

# Instalar dependencias del sistema necesarias
RUN apt-get update && apt-get install -y \
    gcc \
    curl \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# Stage para dependencias
FROM base as dependencies

# Copiar requirements primero para optimizar cache de Docker layers
COPY requirements.txt .

# Instalar dependencias de Python en cache layer separado
RUN pip install --no-cache-dir --upgrade pip \
    && pip install --no-cache-dir -r requirements.txt

# Stage de producci√≥n
FROM base as production

# Copiar dependencias instaladas desde stage anterior
COPY --from=dependencies /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages
COPY --from=dependencies /usr/local/bin /usr/local/bin

# Copiar c√≥digo de la aplicaci√≥n
COPY . .

# Crear usuario no-root para seguridad
RUN useradd --create-home --shell /bin/bash --uid 1000 appuser \
    && chown -R appuser:appuser /app

USER appuser

# Exponer puerto
EXPOSE 8000

# Health check para container orchestration
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Comando para ejecutar la aplicaci√≥n
CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "1"]
```

### Docker para Frontend (React)

#### Dockerfile Multi-stage para Frontend

```dockerfile
# frontend/Dockerfile
# Stage 1: Build
FROM node:18-alpine AS builder

# Establecer directorio de trabajo
WORKDIR /app

# Copiar package files para optimizar cache
COPY package*.json ./
COPY pnpm-lock.yaml* ./

# Instalar pnpm y dependencias
RUN npm install -g pnpm@8 \
    && pnpm install --frozen-lockfile --production=false

# Copiar c√≥digo fuente
COPY . .

# Build optimizado para producci√≥n
RUN pnpm run build

# Stage 2: Production
FROM nginx:alpine AS production

# Instalar curl para health checks
RUN apk add --no-cache curl

# Copiar configuraci√≥n de nginx personalizada
COPY nginx.conf /etc/nginx/nginx.conf

# Copiar build desde el stage anterior
COPY --from=builder /app/dist /usr/share/nginx/html

# Agregar usuario no-root para nginx
RUN addgroup -g 1001 -S nginx \
    && adduser -S -D -H -u 1001 -h /var/cache/nginx -s /sbin/nologin -G nginx -g nginx nginx

# Crear directorios necesarios y dar permisos
RUN mkdir -p /var/cache/nginx /var/log/nginx /var/run \
    && chown -R nginx:nginx /var/cache/nginx /var/log/nginx /var/run \
    && chmod -R 755 /var/cache/nginx /var/log/nginx /var/run

USER nginx

# Exponer puerto
EXPOSE 8080

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8080/health || exit 1

# Comando para ejecutar nginx
CMD ["nginx", "-g", "daemon off;"]
```

#### Configuraci√≥n Nginx Optimizada

```nginx
# frontend/nginx.conf
events {
    worker_connections 1024;
}

http {
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;

    # Logging configuration
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for"';

    access_log /var/log/nginx/access.log main;
    error_log /var/log/nginx/error.log warn;

    # Performance optimizations
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;

    # Gzip compression
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_comp_level 6;
    gzip_types
        text/plain
        text/css
        text/xml
        text/javascript
        application/javascript
        application/xml+rss
        application/json
        application/xml
        image/svg+xml;

    # Security headers
    add_header X-Frame-Options "SAMEORIGIN" always;
    add_header X-Content-Type-Options "nosniff" always;
    add_header X-XSS-Protection "1; mode=block" always;
    add_header Referrer-Policy "no-referrer-when-downgrade" always;
    add_header Content-Security-Policy "default-src 'self' http: https: data: blob: 'unsafe-inline'" always;

    # Rate limiting
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    limit_req_zone $binary_remote_addr zone=login:10m rate=5r/m;

    server {
        listen 8080;
        server_name localhost;
        root /usr/share/nginx/html;
        index index.html;

        # Security configurations
        server_tokens off;
        
        # API proxy with rate limiting
        location /api/ {
            limit_req zone=api burst=20 nodelay;
            proxy_pass http://backend:8000;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            proxy_connect_timeout 30s;
            proxy_send_timeout 30s;
            proxy_read_timeout 30s;
        }

        # Static files with caching
        location / {
            try_files $uri $uri/ /index.html;
            expires 1y;
            add_header Cache-Control "public, immutable";
        }

        # Health check endpoint
        location /health {
            access_log off;
            return 200 "healthy\n";
            add_header Content-Type text/plain;
        }

        # Error pages
        error_page 500 502 503 504 /50x.html;
        location = /50x.html {
            root /usr/share/nginx/html;
        }
    }
}
```

### Docker Compose para Desarrollo

```yaml
# docker-compose.yml
version: '3.8'

services:
  # Base de datos PostgreSQL
  db:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: ecommerce
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ${DB_PASSWORD:-password}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./db/init:/docker-entrypoint-initdb.d
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - app-network

  # Redis para cache y sesiones
  redis:
    image: redis:7-alpine
    command: redis-server --requirepass ${REDIS_PASSWORD:-password}
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - app-network

  # Backend API
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
      target: production
    environment:
      - DATABASE_URL=postgresql://postgres:${DB_PASSWORD:-password}@db:5432/ecommerce
      - REDIS_URL=redis://:${REDIS_PASSWORD:-password}@redis:6379
      - SECRET_KEY=${SECRET_KEY:-dev-secret-key}
      - ENVIRONMENT=development
      - DEBUG=true
    ports:
      - "8000:8000"
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    volumes:
      - ./backend:/app
      - /app/node_modules
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - app-network

  # Frontend
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      target: production
    ports:
      - "3000:8080"
    depends_on:
      backend:
        condition: service_healthy
    environment:
      - REACT_APP_API_URL=http://localhost:8000
    networks:
      - app-network

  # Nginx reverse proxy
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
    depends_on:
      - frontend
      - backend
    networks:
      - app-network

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local

networks:
  app-network:
    driver: bridge
```

## CI/CD Pipelines

### GitHub Actions para Backend

```yaml
# .github/workflows/backend.yml
name: Backend CI/CD

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'backend/**'
  pull_request:
    branches: [ main ]
    paths:
      - 'backend/**'

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}/backend

jobs:
  test:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Install dependencies
      run: |
        cd backend
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-asyncio pytest-cov bandit safety

    - name: Run linting
      run: |
        cd backend
        python -m flake8 src/ --max-line-length=100
        python -m black --check src/
        python -m isort --check-only src/

    - name: Run security checks
      run: |
        cd backend
        bandit -r src/ -f json -o bandit-report.json
        safety check --json --output safety-report.json
      continue-on-error: true

    - name: Run tests
      env:
        DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_db
        REDIS_URL: redis://localhost:6379
        SECRET_KEY: test-secret-key
      run: |
        cd backend
        pytest tests/ -v --cov=src --cov-report=xml --cov-report=html --cov-fail-under=80

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./backend/coverage.xml
        flags: backend
        name: backend-coverage

    - name: Upload test artifacts
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: test-results
        path: |
          backend/htmlcov/
          backend/bandit-report.json
          backend/safety-report.json

  security-scan:
    runs-on: ubuntu-latest
    needs: test
    if: github.event_name == 'push'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: './backend'
        format: 'sarif'
        output: 'trivy-results.sarif'

    - name: Upload Trivy scan results
      uses: github/codeql-action/upload-sarif@v2
      if: always()
      with:
        sarif_file: 'trivy-results.sarif'

  build-and-push:
    runs-on: ubuntu-latest
    needs: [test, security-scan]
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=semver,pattern={{version}}
          type=semver,pattern={{major}}.{{minor}}
          type=sha,prefix={{branch}}-

    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: ./backend
        platforms: linux/amd64,linux/arm64
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  deploy:
    runs-on: ubuntu-latest
    needs: build-and-push
    if: github.ref == 'refs/heads/main'
    environment: production
    
    steps:
    - name: Deploy to production
      run: |
        echo "Deploying to production environment"
        # Aqu√≠ ir√≠a la l√≥gica de deployment espec√≠fica
```

### GitHub Actions para Frontend

```yaml
# .github/workflows/frontend.yml
name: Frontend CI/CD

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'frontend/**'
  pull_request:
    branches: [ main ]
    paths:
      - 'frontend/**'

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}/frontend

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'

    - name: Install pnpm
      uses: pnpm/action-setup@v2
      with:
        version: 8

    - name: Get pnpm store directory
      shell: bash
      run: |
        echo "STORE_PATH=$(pnpm store path --silent)" >> $GITHUB_ENV

    - name: Setup pnpm cache
      uses: actions/cache@v3
      with:
        path: ${{ env.STORE_PATH }}
        key: ${{ runner.os }}-pnpm-store-${{ hashFiles('**/pnpm-lock.yaml') }}
        restore-keys: |
          ${{ runner.os }}-pnpm-store-

    - name: Install dependencies
      run: |
        cd frontend
        pnpm install --frozen-lockfile

    - name: Run linting
      run: |
        cd frontend
        pnpm run lint
        pnpm run type-check

    - name: Run tests
      run: |
        cd frontend
        pnpm run test:ci

    - name: Build application
      run: |
        cd frontend
        pnpm run build

    - name: Upload build artifacts
      uses: actions/upload-artifact@v3
      with:
        name: frontend-build
        path: frontend/dist

  lighthouse:
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'

    - name: Download build artifacts
      uses: actions/download-artifact@v3
      with:
        name: frontend-build
        path: frontend/dist

    - name: Setup Lighthouse CI
      run: |
        npm install -g @lhci/cli@0.12.x

    - name: Run Lighthouse CI
      env:
        LHCI_GITHUB_APP_TOKEN: ${{ secrets.LHCI_GITHUB_APP_TOKEN }}
      run: |
        cd frontend
        lhci autorun

  build-and-push:
    runs-on: ubuntu-latest
    needs: [test, lighthouse]
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=semver,pattern={{version}}
          type=semver,pattern={{major}}.{{minor}}
          type=sha,prefix={{branch}}-

    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: ./frontend
        platforms: linux/amd64,linux/arm64
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max
```

## Cloud Deployment

### Terraform para AWS Infrastructure

```hcl
# infrastructure/aws/main.tf
terraform {
  required_version = ">= 1.0"
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

provider "aws" {
  region = var.aws_region
}

# VPC Configuration
resource "aws_vpc" "main" {
  cidr_block           = "10.0.0.0/16"
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = "${var.project_name}-vpc"
  }
}

# Internet Gateway
resource "aws_internet_gateway" "main" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name = "${var.project_name}-igw"
  }
}

# Public Subnets
resource "aws_subnet" "public" {
  count                   = length(var.availability_zones)
  vpc_id                  = aws_vpc.main.id
  cidr_block              = "10.0.${count.index + 1}.0/24"
  availability_zone       = var.availability_zones[count.index]
  map_public_ip_on_launch = true

  tags = {
    Name = "${var.project_name}-public-${count.index + 1}"
  }
}

# Private Subnets
resource "aws_subnet" "private" {
  count             = length(var.availability_zones)
  vpc_id            = aws_vpc.main.id
  cidr_block        = "10.0.${count.index + 10}.0/24"
  availability_zone = var.availability_zones[count.index]

  tags = {
    Name = "${var.project_name}-private-${count.index + 1}"
  }
}

# Route Table for Public Subnets
resource "aws_route_table" "public" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.main.id
  }

  tags = {
    Name = "${var.project_name}-public-rt"
  }
}

# Route Table Association for Public Subnets
resource "aws_route_table_association" "public" {
  count          = length(aws_subnet.public)
  subnet_id      = aws_subnet.public[count.index].id
  route_table_id = aws_route_table.public.id
}

# Security Group for ALB
resource "aws_security_group" "alb" {
  name        = "${var.project_name}-alb-sg"
  description = "Security group for Application Load Balancer"
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "${var.project_name}-alb-sg"
  }
}

# Application Load Balancer
resource "aws_lb" "main" {
  name               = "${var.project_name}-alb"
  internal           = false
  load_balancer_type = "application"
  security_groups    = [aws_security_group.alb.id]
  subnets            = aws_subnet.public[*].id

  enable_deletion_protection = false

  tags = {
    Name = "${var.project_name}-alb"
  }
}

# ECS Cluster
resource "aws_ecs_cluster" "main" {
  name = "${var.project_name}-cluster"

  setting {
    name  = "containerInsights"
    value = "enabled"
  }

  tags = {
    Name = "${var.project_name}-cluster"
  }
}

# RDS Subnet Group
resource "aws_db_subnet_group" "main" {
  name       = "${var.project_name}-db-subnet-group"
  subnet_ids = aws_subnet.private[*].id

  tags = {
    Name = "${var.project_name}-db-subnet-group"
  }
}

# Security Group for RDS
resource "aws_security_group" "rds" {
  name        = "${var.project_name}-rds-sg"
  description = "Security group for RDS database"
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port       = 5432
    to_port         = 5432
    protocol        = "tcp"
    security_groups = [aws_security_group.ecs.id]
  }

  tags = {
    Name = "${var.project_name}-rds-sg"
  }
}

# RDS Database Instance
resource "aws_db_instance" "main" {
  identifier             = "${var.project_name}-db"
  engine                 = "postgres"
  engine_version         = "15.4"
  instance_class         = "db.t3.micro"
  allocated_storage      = 20
  max_allocated_storage  = 100
  storage_type           = "gp2"
  storage_encrypted      = true
  
  db_name  = var.db_name
  username = var.db_username
  password = var.db_password
  
  vpc_security_group_ids = [aws_security_group.rds.id]
  db_subnet_group_name   = aws_db_subnet_group.main.name
  
  backup_retention_period = 7
  backup_window          = "03:00-04:00"
  maintenance_window     = "sun:04:00-sun:05:00"
  
  skip_final_snapshot = true
  deletion_protection = false

  tags = {
    Name = "${var.project_name}-db"
  }
}

# Variables
variable "aws_region" {
  description = "AWS region"
  type        = string
  default     = "us-east-1"
}

variable "project_name" {
  description = "Name of the project"
  type        = string
  default     = "ecommerce"
}

variable "availability_zones" {
  description = "List of availability zones"
  type        = list(string)
  default     = ["us-east-1a", "us-east-1b"]
}

variable "db_name" {
  description = "Database name"
  type        = string
  default     = "ecommerce"
}

variable "db_username" {
  description = "Database username"
  type        = string
  default     = "postgres"
}

variable "db_password" {
  description = "Database password"
  type        = string
  sensitive   = true
}
```

## Monitoring y Alertas

### Configuraci√≥n de Prometheus

```yaml
# monitoring/prometheus/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    cluster: 'ecommerce'
    replica: 'prometheus-1'

rule_files:
  - "alert_rules.yml"
  - "recording_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  # Prometheus self-monitoring
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  # Backend application metrics
  - job_name: 'backend'
    static_configs:
      - targets: ['backend:8000']
    metrics_path: '/metrics'
    scrape_interval: 10s
    scrape_timeout: 5s

  # Frontend/Nginx metrics
  - job_name: 'nginx'
    static_configs:
      - targets: ['frontend:8080']
    metrics_path: '/nginx_status'
    scrape_interval: 15s

  # Database metrics
  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres_exporter:9187']
    scrape_interval: 30s

  # Redis metrics
  - job_name: 'redis'
    static_configs:
      - targets: ['redis_exporter:9121']
    scrape_interval: 15s

  # Node/Container metrics
  - job_name: 'node'
    static_configs:
      - targets: ['node_exporter:9100']
    scrape_interval: 15s

  # Docker container metrics
  - job_name: 'cadvisor'
    static_configs:
      - targets: ['cadvisor:8080']
    scrape_interval: 15s
```

### Reglas de Alertas

```yaml
# monitoring/prometheus/alert_rules.yml
groups:
- name: ecommerce.rules
  rules:
  # High error rate
  - alert: HighErrorRate
    expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.1
    for: 5m
    labels:
      severity: warning
      service: "{{ $labels.job }}"
    annotations:
      summary: "High error rate detected"
      description: "Error rate is {{ $value }} errors per second on {{ $labels.job }}"

  # High response time
  - alert: HighResponseTime
    expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1
    for: 2m
    labels:
      severity: warning
      service: "{{ $labels.job }}"
    annotations:
      summary: "High response time detected"
      description: "95th percentile response time is {{ $value }}s on {{ $labels.job }}"

  # Database connection issues
  - alert: DatabaseDown
    expr: up{job="postgres"} == 0
    for: 1m
    labels:
      severity: critical
      service: database
    annotations:
      summary: "Database is down"
      description: "PostgreSQL database has been down for more than 1 minute"

  # High memory usage
  - alert: HighMemoryUsage
    expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes > 0.9
    for: 5m
    labels:
      severity: warning
      service: infrastructure
    annotations:
      summary: "High memory usage detected"
      description: "Memory usage is above 90% for 5 minutes"

  # High CPU usage
  - alert: HighCPUUsage
    expr: 100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
    for: 5m
    labels:
      severity: warning
      service: infrastructure
    annotations:
      summary: "High CPU usage detected"
      description: "CPU usage is above 80% for 5 minutes on {{ $labels.instance }}"

  # Service down
  - alert: ServiceDown
    expr: up == 0
    for: 1m
    labels:
      severity: critical
      service: "{{ $labels.job }}"
    annotations:
      summary: "Service is down"
      description: "{{ $labels.job }} service has been down for more than 1 minute"
```

### Dashboard de Grafana

```json
{
  "dashboard": {
    "id": null,
    "title": "E-commerce Production Dashboard",
    "tags": ["ecommerce", "production"],
    "timezone": "browser",
    "refresh": "30s",
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "panels": [
      {
        "id": 1,
        "title": "Request Rate",
        "type": "graph",
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0},
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])",
            "legendFormat": "{{method}} {{path}}",
            "refId": "A"
          }
        ],
        "yAxes": [
          {
            "label": "requests/sec",
            "min": 0
          }
        ]
      },
      {
        "id": 2,
        "title": "Error Rate",
        "type": "graph",
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0},
        "targets": [
          {
            "expr": "rate(http_requests_total{status=~\"4..|5..\"}[5m])",
            "legendFormat": "{{status}} errors",
            "refId": "A"
          }
        ],
        "alert": {
          "conditions": [
            {
              "evaluator": {
                "params": [0.1],
                "type": "gt"
              },
              "operator": {
                "type": "and"
              },
              "query": {
                "params": ["A", "5m", "now"]
              },
              "reducer": {
                "type": "avg"
              },
              "type": "query"
            }
          ],
          "executionErrorState": "alerting",
          "for": "2m",
          "frequency": "30s",
          "handler": 1,
          "name": "High Error Rate",
          "noDataState": "no_data",
          "notifications": []
        }
      },
      {
        "id": 3,
        "title": "Response Time (95th percentile)",
        "type": "graph",
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8},
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile",
            "refId": "A"
          },
          {
            "expr": "histogram_quantile(0.50, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "50th percentile",
            "refId": "B"
          }
        ]
      },
      {
        "id": 4,
        "title": "Active Users",
        "type": "singlestat",
        "gridPos": {"h": 4, "w": 6, "x": 12, "y": 8},
        "targets": [
          {
            "expr": "active_users_total",
            "refId": "A"
          }
        ],
        "valueName": "current"
      }
    ]
  }
}
```

## Security y Secrets Management

### Configuraci√≥n de Secretos

```yaml
# .env.production (ejemplo - NO commitear al repo)
# Database
DATABASE_URL=postgresql://user:${DB_PASSWORD}@prod-db:5432/ecommerce
POSTGRES_DB=ecommerce
POSTGRES_USER=ecommerce_user
POSTGRES_PASSWORD=${DB_PASSWORD}

# Security
SECRET_KEY=${JWT_SECRET_KEY}
JWT_SECRET=${JWT_SECRET_KEY}
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30

# Redis
REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379

# External APIs
STRIPE_API_KEY=${STRIPE_API_KEY}
STRIPE_WEBHOOK_SECRET=${STRIPE_WEBHOOK_SECRET}
SENDGRID_API_KEY=${SENDGRID_API_KEY}

# Monitoring
PROMETHEUS_ENABLED=true
GRAFANA_URL=https://monitoring.example.com

# SSL/TLS
SSL_CERT_PATH=/etc/ssl/certs/cert.pem
SSL_KEY_PATH=/etc/ssl/private/key.pem

# CORS
CORS_ORIGINS=https://example.com,https://www.example.com
```

### Docker Secrets Management

```yaml
# docker-compose.prod.yml
version: '3.8'

services:
  backend:
    image: ghcr.io/username/ecommerce-backend:latest
    environment:
      - DATABASE_URL_FILE=/run/secrets/db_url
      - JWT_SECRET_FILE=/run/secrets/jwt_secret
    secrets:
      - db_url
      - jwt_secret
    deploy:
      replicas: 3
      update_config:
        parallelism: 1
        delay: 10s
        order: stop-first
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3

secrets:
  db_url:
    external: true
  jwt_secret:
    external: true
  stripe_key:
    external: true
```

## Ejercicios Pr√°cticos

### Ejercicio 1: Docker Containerizaci√≥n

<CardGrid>
  <Card title="üéØ Objetivo" icon="star">
    Containerizar el proyecto e-commerce completo con Docker para desarrollo y producci√≥n
  </Card>
  
  <Card title="‚úÖ Criterios de Evaluaci√≥n" icon="approve-check">
    - Backend containerizado y funcional
    - Frontend containerizado y funcional
    - Docker Compose ejecut√°ndose correctamente
    - Health checks implementados
    - Im√°genes optimizadas
  </Card>
</CardGrid>

#### Prompts para Cursor AI

```bash
# 1. Crear Dockerfile Backend
@cursor: "Crea un Dockerfile multi-stage para el backend del e-commerce usando Python 3.11. Incluye optimizaciones, health check, usuario no-root para seguridad, y stages para development/production"

# 2. Crear Dockerfile Frontend
@cursor: "Crea un Dockerfile multi-stage para el frontend del e-commerce usando Node.js 18. Incluye build stage con pnpm, nginx para producci√≥n, optimizaci√≥n de assets, y security headers"

# 3. Configurar Docker Compose
@cursor: "Crea un docker-compose.yml completo para el e-commerce que incluya backend, frontend, PostgreSQL, Redis, nginx como reverse proxy, volumes persistentes, y health checks"

# 4. Optimizar Im√°genes
@cursor: "Optimiza los Dockerfiles del e-commerce para reducir el tama√±o de imagen. Usa multi-stage builds, .dockerignore, Alpine Linux, y mejores pr√°cticas de security"
```

#### Estructura Final Esperada

```bash
ecommerce/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ .dockerignore
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ .dockerignore
‚îÇ   ‚îî‚îÄ‚îÄ nginx.conf
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ docker-compose.prod.yml
‚îî‚îÄ‚îÄ nginx/
    ‚îú‚îÄ‚îÄ nginx.conf
    ‚îî‚îÄ‚îÄ ssl/
```

### Ejercicio 2: CI/CD Pipeline

<CardGrid>
  <Card title="üîÑ Objetivo" icon="rocket">
    Implementar pipeline CI/CD completo para el proyecto e-commerce
  </Card>
  
  <Card title="üéØ Resultado Esperado" icon="approve-check">
    - Pipeline backend funcional con tests
    - Pipeline frontend funcional con Lighthouse
    - Security scanning implementado
    - Deployment autom√°tico configurado
  </Card>
</CardGrid>

#### Prompts para Cursor AI

```bash
# 1. Crear GitHub Actions Backend
@cursor: "Crea un workflow de GitHub Actions para el backend del e-commerce. Incluye testing con pytest, security scan con bandit/safety, build Docker multi-platform, push a GHCR, y deployment autom√°tico a AWS ECS"

# 2. Crear GitHub Actions Frontend
@cursor: "Crea un workflow de GitHub Actions para el frontend del e-commerce. Incluye linting con ESLint, testing con Jest, build optimizado, Lighthouse CI, build Docker, y deployment a S3/CloudFront"

# 3. Configurar Secrets
@cursor: "Lista y configura todos los secrets necesarios para GitHub Actions del e-commerce. Incluye AWS credentials, Docker registry tokens, y variables de entorno production"
```

### Ejercicio 3: Cloud Deployment con Terraform

<CardGrid>
  <Card title="‚òÅÔ∏è Objetivo" icon="document">
    Desplegar el e-commerce en AWS usando Infrastructure as Code
  </Card>
  
  <Card title="üìä M√©tricas Objetivo" icon="chart">
    - Infraestructura como c√≥digo funcional
    - Deployment en AWS exitoso
    - Auto-scaling configurado
    - Monitoring implementado
  </Card>
</CardGrid>

#### Prompts para Cursor AI

```bash
# 1. Crear Terraform para AWS
@cursor: "Crea configuraci√≥n Terraform completa para desplegar el e-commerce en AWS. Incluye VPC con subnets p√∫blicas/privadas, ECS Fargate cluster, RDS PostgreSQL, ALB, auto-scaling, y security groups optimizados"

# 2. Configurar ECS Services
@cursor: "Configura servicios ECS para el e-commerce usando Terraform. Incluye task definitions para backend/frontend, service discovery, auto-scaling policies, health checks, y rolling deployments"
```

## M√©tricas de Transformaci√≥n

| M√©trica | Antes | Despu√©s | Mejora |
|---------|--------|---------|---------|
| **Deployment Time** | 2 horas manual | 5 minutos autom√°tico | 24x m√°s r√°pido |
| **Uptime** | 95% | 99.9% | 5x m√°s confiable |
| **Error Detection** | Manual | Autom√°tico | ‚àû |
| **Scaling** | Manual | Auto-scaling | ‚àû |
| **Monitoring** | B√°sico | Observabilidad completa | 10x |
| **Security** | Ad-hoc | Automated scanning | Enterprise-grade |

### Skills Desarrollados

- ‚úÖ **Docker containerizaci√≥n profesional**
- ‚úÖ **CI/CD pipelines automatizados**
- ‚úÖ **Cloud deployment (AWS/GCP/Azure)**
- ‚úÖ **Infrastructure as Code (Terraform)**
- ‚úÖ **Monitoring y observabilidad**
- ‚úÖ **Security y secrets management**
- ‚úÖ **Auto-scaling y load balancing**
- ‚úÖ **Disaster recovery strategies**

:::tip[Filosof√≠a de DevOps con Cursor AI]
**Regla de Oro**: "Automatiza todo lo que sea repetible, monitorea todo lo que sea cr√≠tico"

**‚ùå MAL**: "Haz un Dockerfile"

**‚úÖ BIEN**: "Crea un Dockerfile multi-stage para el backend del e-commerce usando Python 3.11, incluye health check, usuario no-root, optimizaciones de seguridad, y stages para development/production"
:::

## Troubleshooting de DevOps Com√∫n

### Problemas Frecuentes y Soluciones

#### Contenedores no inician correctamente
```bash
# Verificar logs de container
docker logs <container_id> --tail 50
# Verificar health checks
docker inspect <container_id> | grep Health -A 10
```

#### Pipeline CI/CD falla en tests
1. Verificar variables de entorno en GitHub Secrets
2. Confirmar que servicios de testing (DB, Redis) est√°n healthy
3. Revisar versiones de dependencias en cache

#### Deployment falla en producci√≥n
```bash
# Verificar estado de servicios ECS
aws ecs describe-services --cluster prod --services backend
# Verificar logs de CloudWatch
aws logs tail /aws/ecs/backend --follow
```

:::tip[M√°s Ayuda]
Para m√°s problemas de DevOps, consulta [DevOps Troubleshooting](https://docs.cursor.com/troubleshooting/devops).
:::

## Pr√≥ximo Paso

¬°Has completado el M√≥dulo 6! Ahora tienes las habilidades para desplegar y mantener aplicaciones empresariales en producci√≥n. Has transformado tu proyecto e-commerce en un **sistema completo enterprise-ready** con Docker, CI/CD, cloud deployment, monitoring avanzado y pr√°cticas de DevOps profesionales.

üéâ **¬°Felicitaciones!** Has completado el **Curso Completo de Cursor AI para Desarrolladores**. Ahora tienes todas las herramientas y conocimientos para desarrollar aplicaciones modernas de nivel empresarial usando Cursor AI como tu copiloto de desarrollo.